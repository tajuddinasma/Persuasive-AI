{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Agents with burden of proof"
      ],
      "metadata": {
        "id": "AGMXAV-24zJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.burden_of_proof = False\n",
        "\n",
        "    def make_claim(self, other, claim):\n",
        "        print(f\"{self.name} to {other.name}: {claim}\")\n",
        "        self.burden_of_proof = True\n",
        "        other.receive_claim(self, claim)\n",
        "\n",
        "    def receive_claim(self, other, claim):\n",
        "        print(f\"{self.name} received: {claim} from {other.name}\")\n",
        "        if other.burden_of_proof:\n",
        "            print(f\"{other.name} has the burden of proof.\")\n",
        "        else:\n",
        "            print(f\"{self.name} has the burden of proof.\")\n",
        "\n",
        "    def provide_evidence(self, other, evidence):\n",
        "        print(f\"{self.name} to {other.name}: {evidence}\")\n",
        "        self.burden_of_proof = False\n",
        "        other.receive_evidence(self, evidence)\n",
        "\n",
        "    def receive_evidence(self, other, evidence):\n",
        "        print(f\"{self.name} received: {evidence} from {other.name}\")\n",
        "        if other.burden_of_proof:\n",
        "            print(f\"{other.name} still has the burden of proof.\")\n",
        "        else:\n",
        "            print(f\"The burden of proof has been satisfied.\")\n",
        "\n",
        "# Create two agents\n",
        "agent1 = Agent(\"Agent1\")\n",
        "agent2 = Agent(\"Agent2\")\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "agent1.make_claim(agent2, \"We should go to the park.\")\n",
        "agent1.provide_evidence(agent2, \"The weather is nice.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek-u2hmYnTIv",
        "outputId": "4029de0a-09d8-48de-c528-769c8101b5cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent1 to Agent2: We should go to the park.\n",
            "Agent2 received: We should go to the park. from Agent1\n",
            "Agent1 has the burden of proof.\n",
            "Agent1 to Agent2: The weather is nice.\n",
            "Agent2 received: The weather is nice. from Agent1\n",
            "The burden of proof has been satisfied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods"
      ],
      "metadata": {
        "id": "r1xhxOKf5Bpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def make_claim(self, other, claim):\n",
        "        print(f\"{self.name} to {other.name}: {claim}\")\n",
        "        self.burden_of_proof = True\n",
        "        other.receive_claim(self, claim)\n",
        "\n",
        "    def provide_evidence(self, other, evidence):\n",
        "        print(f\"{self.name} to {other.name}: {evidence}\")\n",
        "        self.burden_of_proof = False\n",
        "        other.receive_evidence(self, evidence)"
      ],
      "metadata": {
        "id": "g2GEYLgCnYMv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def receive_claim(self, other, claim):\n",
        "        print(f\"{self.name} received: {claim} from {other.name}\")\n",
        "        if other.burden_of_proof:\n",
        "            print(f\"{other.name} has the burden of proof.\")\n",
        "        else:\n",
        "            print(f\"{self.name} has the burden of proof.\")\n",
        "\n",
        "    def receive_evidence(self, other, evidence):\n",
        "        print(f\"{self.name} received: {evidence} from {other.name}\")\n",
        "        if other.burden_of_proof:\n",
        "            print(f\"{other.name} still has the burden of proof.\")\n",
        "        else:\n",
        "            print(f\"The burden of proof has been satisfied.\")"
      ],
      "metadata": {
        "id": "OBDn-ZGYnaMm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating two agents"
      ],
      "metadata": {
        "id": "1bU6jJMv5GAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two agents\n",
        "agent1 = Agent(\"Agent1\")\n",
        "agent2 = Agent(\"Agent2\")\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "agent1.make_claim(agent2, \"We should go to the park.\")\n",
        "agent1.provide_evidence(agent2, \"The weather is nice.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsftIoJQnc2o",
        "outputId": "5d451ccf-882a-4178-9c0f-82efd38780ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent1 to Agent2: We should go to the park.\n",
            "Agent2 received: We should go to the park. from Agent1\n",
            "Agent1 has the burden of proof.\n",
            "Agent1 to Agent2: The weather is nice.\n",
            "Agent2 received: The weather is nice. from Agent1\n",
            "The burden of proof has been satisfied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents with chat features"
      ],
      "metadata": {
        "id": "GKWwdyBc5KtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.burden_of_proof = False\n",
        "\n",
        "    def make_claim(self, claim):\n",
        "        self.burden_of_proof = True\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self, evidence):\n",
        "        self.burden_of_proof = False\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, agent1, agent2):\n",
        "        self.agent1 = agent1\n",
        "        self.agent2 = agent2\n",
        "\n",
        "    def start_dialogue(self, claim, evidence):\n",
        "        print(self.agent1.make_claim(claim))\n",
        "        print(self.receive_claim(claim))\n",
        "        print(self.agent1.provide_evidence(evidence))\n",
        "        print(self.receive_evidence(evidence))\n",
        "\n",
        "    def receive_claim(self, claim):\n",
        "        if self.agent1.burden_of_proof:\n",
        "            return f\"{self.agent2.name} received: {claim} from {self.agent1.name}. {self.agent1.name} has the burden of proof.\"\n",
        "        else:\n",
        "            return f\"{self.agent2.name} received: {claim} from {self.agent1.name}. {self.agent2.name} has the burden of proof.\"\n",
        "\n",
        "    def receive_evidence(self, evidence):\n",
        "        if self.agent1.burden_of_proof:\n",
        "            return f\"{self.agent2.name} received: {evidence} from {self.agent1.name}. {self.agent1.name} still has the burden of proof.\"\n",
        "        else:\n",
        "            return f\"{self.agent2.name} received: {evidence} from {self.agent1.name}. The burden of proof has been satisfied.\"\n",
        "\n",
        "# Create two agents\n",
        "agent1 = Agent(\"Agent1\")\n",
        "agent2 = Agent(\"Agent2\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(agent1, agent2)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue(\"We should go to the park.\", \"The weather is nice.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsWJq8Zhn-f0",
        "outputId": "39023c4f-58fd-4bdc-d6a7-889873e40ea1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent1: We should go to the park.\n",
            "Agent2 received: We should go to the park. from Agent1. Agent1 has the burden of proof.\n",
            "Agent1: The weather is nice.\n",
            "Agent2 received: The weather is nice. from Agent1. The burden of proof has been satisfied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User class"
      ],
      "metadata": {
        "id": "LM_1wcPq5P8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class User:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def make_claim(self):\n",
        "        claim = input(f\"{self.name}, what is your claim? \")\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self):\n",
        "        evidence = input(f\"{self.name}, what is your evidence? \")\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.burden_of_proof = False\n",
        "\n",
        "    def receive_claim(self, claim):\n",
        "        self.burden_of_proof = True\n",
        "        return f\"{self.name} received: {claim}. {self.name} has the burden of proof.\"\n",
        "\n",
        "    def receive_evidence(self, evidence):\n",
        "        self.burden_of_proof = False\n",
        "        return f\"{self.name} received: {evidence}. The burden of proof has been satisfied.\"\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, user, agent):\n",
        "        self.user = user\n",
        "        self.agent = agent\n",
        "\n",
        "    def start_dialogue(self):\n",
        "        claim = self.user.make_claim()\n",
        "        print(claim)\n",
        "        print(self.agent.receive_claim(claim))\n",
        "        evidence = self.user.provide_evidence()\n",
        "        print(evidence)\n",
        "        print(self.agent.receive_evidence(evidence))\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLjG5WNdoNZA",
        "outputId": "a9933296-f8f9-4a73-f73b-8d4afc31aa69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User, what is your claim? Sky is pink\n",
            "User: Sky is pink\n",
            "Agent received: User: Sky is pink. Agent has the burden of proof.\n",
            "User, what is your evidence? I saw it\n",
            "User: I saw it\n",
            "Agent received: User: I saw it. The burden of proof has been satisfied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent with set of fixed responses"
      ],
      "metadata": {
        "id": "uIGml-2G5SxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class User:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def make_claim(self):\n",
        "        claim = input(f\"{self.name}, what is your claim? \")\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self):\n",
        "        evidence = input(f\"{self.name}, what is your evidence? \")\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.burden_of_proof = False\n",
        "        self.responses = [\"Interesting, but I need more evidence.\",\n",
        "                          \"I see, but can you provide more details?\",\n",
        "                          \"That's a good point, but I'm not entirely convinced yet.\"]\n",
        "\n",
        "    def receive_claim(self, claim):\n",
        "        self.burden_of_proof = True\n",
        "        return f\"{self.name} received: {claim}. {self.name} has the burden of proof.\"\n",
        "\n",
        "    def receive_evidence(self, evidence):\n",
        "        self.burden_of_proof = False\n",
        "        return f\"{self.name} received: {evidence}. {random.choice(self.responses)}\"\n",
        "\n",
        "    def make_counter_claim(self):\n",
        "        counter_claim = \"However, I believe we should consider other factors as well.\"\n",
        "        return f\"{self.name}: {counter_claim}\"\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, user, agent):\n",
        "        self.user = user\n",
        "        self.agent = agent\n",
        "\n",
        "    def start_dialogue(self):\n",
        "        claim = self.user.make_claim()\n",
        "        print(claim)\n",
        "        print(self.agent.receive_claim(claim))\n",
        "        evidence = self.user.provide_evidence()\n",
        "        print(evidence)\n",
        "        print(self.agent.receive_evidence(evidence))\n",
        "        print(self.agent.make_counter_claim())\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elBOXqFBpcv9",
        "outputId": "2f13a0a2-53eb-4456-9afd-cf23fa67ae70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User, what is your claim? Sky is pink\n",
            "User: Sky is pink\n",
            "Agent received: User: Sky is pink. Agent has the burden of proof.\n",
            "User, what is your evidence? I saw it\n",
            "User: I saw it\n",
            "Agent received: User: I saw it. That's a good point, but I'm not entirely convinced yet.\n",
            "Agent: However, I believe we should consider other factors as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 for response generation, the agent here is gpt-2"
      ],
      "metadata": {
        "id": "WtVgX-tI5cGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "class User:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def make_claim(self):\n",
        "        claim = input(f\"{self.name}, what is your claim? \")\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self):\n",
        "        evidence = input(f\"{self.name}, what is your evidence? \")\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.burden_of_proof = False\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        outputs = self.model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
        "        response = self.tokenizer.decode(outputs[:, inputs.shape[-1]:][0], skip_special_tokens=True)\n",
        "        return f\"{self.name}: {response}\"\n",
        "\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, user, agent):\n",
        "        self.user = user\n",
        "        self.agent = agent\n",
        "\n",
        "    def start_dialogue(self):\n",
        "        claim = self.user.make_claim()\n",
        "        print(claim)\n",
        "        print(self.agent.generate_response(claim))\n",
        "        evidence = self.user.provide_evidence()\n",
        "        print(evidence)\n",
        "        print(self.agent.generate_response(evidence))\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MycAkLB6psO2",
        "outputId": "77b30faf-20d2-4cc4-f2ad-45b62cd90c19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User, what is your claim? global warming is not real\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: global warming is not real\n",
            "Agent: , but it is happening.\n",
            "\n",
            "Anonymous: I'm not sure what to say. I think it's a good thing that we're all aware of it. It's not a problem. We're just not aware. The problem is that it has been happening for a long time. And it will continue to happen. So I don't know what the problem will be. But I do know that the world is warming. That's what I believe. If\n",
            "User, what is your evidence? Earth has gone in cycles of cooling and warming\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Earth has gone in cycles of cooling and warming\n",
            "Agent: . The Earth's surface has warmed by about 1.5 degrees Celsius (3.6 degrees Fahrenheit) since the last ice age.\n",
            "\n",
            "The Earth is now about 2.8 degrees warmer than it was in the past. This is because the Earth was about to become a warmer place. It is also because of the fact that the planet has been cooling for about a century. In fact, the temperature of Earth since its formation has risen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For google colab"
      ],
      "metadata": {
        "id": "bxDVA-7_5mvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing-extensions==4.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3f8gDgdvOKl",
        "outputId": "fee6b1d4-6bbf-43f0-fee4-be0a1da40aea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions==4.7.0\n",
            "  Downloading typing_extensions-4.7.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "keeps changing: saved it for future changes"
      ],
      "metadata": {
        "id": "mZ79stxf5vcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install typing-extensions==4.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-6zBcGWur5R",
        "outputId": "787a54f2-2ce3-4664-b90d-b7587f47acd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions==4.5.0\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.6.1 requires typing-extensions<5,>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works for open ai"
      ],
      "metadata": {
        "id": "K_zk8mSt52Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing-extensions==3.10.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "vu2h6jArvqjB",
        "outputId": "c27b01f2-5492-4717-8909-b2bc0d721416"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions==3.10.0.2\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.0\n",
            "    Uninstalling typing_extensions-4.7.0:\n",
            "      Successfully uninstalled typing_extensions-4.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.23 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "flax 0.7.5 requires typing-extensions>=4.2, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "ibis-framework 6.2.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "librosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "openai 1.6.1 requires typing-extensions<5,>=4.7, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "pydantic 1.10.13 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "python-utils 3.8.1 requires typing-extensions>3.10.0.2, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "some features are not compatible, first use this, in case of error, use the other one below"
      ],
      "metadata": {
        "id": "JBAxNsJ055Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Trw8UnBr76v",
        "outputId": "e0b4bf4b-86bc-4475-b179-c8c14555cbf5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this works"
      ],
      "metadata": {
        "id": "0y0BEsaY6GzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "rpz-Uz6-y8nq",
        "outputId": "d5e43d47-2ae1-467f-e777-d2e8320afbeb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m61.4/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.6.1\n",
            "    Uninstalling openai-1.6.1:\n",
            "      Successfully uninstalled openai-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For limited responses, here the agent is gpt-3 turbo"
      ],
      "metadata": {
        "id": "LIZ6nj-T6OWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "\n",
        "class User:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def make_claim(self):\n",
        "        claim = input(f\"{self.name}, what is your claim? \")\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self):\n",
        "        evidence = input(f\"{self.name}, what is your evidence? \")\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return f\"{self.name}: {response['choices'][0]['message']['content']}\"\\\n",
        "\n",
        "\n",
        "def generate_response(self, prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return f\"{self.name}: {response['choices'][0]['message']['content']}\"\n",
        "\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, user, agent):\n",
        "        self.user = user\n",
        "        self.agent = agent\n",
        "\n",
        "    def start_dialogue(self):\n",
        "        claim = self.user.make_claim()\n",
        "        print(claim)\n",
        "        print(self.agent.generate_response(claim))\n",
        "        evidence = self.user.provide_evidence()\n",
        "        print(evidence)\n",
        "        print(self.agent.generate_response(evidence))\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxFCst16p64C",
        "outputId": "7d1755ca-9cb6-4060-8817-646ec02e6a53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User, what is your claim? lochness monster exists\n",
            "User: lochness monster exists\n",
            "Agent: Assistant: The Loch Ness Monster is a creature of cryptozoology, which is the study of hidden or unknown animals. There have been numerous reports and sightings of a creature in Loch Ness, a large freshwater lake in Scotland. However, despite extensive searches and scientific investigations, no conclusive evidence has been found to prove the existence of the Loch Ness Monster. Many of the supposed sightings and photographs over the years have been debunked or proven to be hoaxes. It remains an intriguing mystery that captures the imagination of many people, but its existence is still highly debated and unproven.\n",
            "User, what is your evidence? Someone sighted it\n",
            "User: Someone sighted it\n",
            "Agent: Assistant: Could you please provide more context or information about what \"it\" refers to?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Responses are limited to 10"
      ],
      "metadata": {
        "id": "Uct5K19K6W1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "class User:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def make_claim(self):\n",
        "        claim = input(f\"{self.name}, what is your claim? \")\n",
        "        return f\"{self.name}: {claim}\"\n",
        "\n",
        "    def provide_evidence(self):\n",
        "        evidence = input(f\"{self.name}, what is your evidence? \")\n",
        "        return f\"{self.name}: {evidence}\"\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def generate_response(self, prompt):\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return f\"{self.name}: {response['choices'][0]['message']['content']}\"\n",
        "\n",
        "class Chat:\n",
        "    def __init__(self, user, agent):\n",
        "        self.user = user\n",
        "        self.agent = agent\n",
        "\n",
        "    def start_dialogue(self):\n",
        "        claim = self.user.make_claim()\n",
        "        print(claim)\n",
        "        print(self.agent.generate_response(claim))\n",
        "\n",
        "        for _ in range(10):  # Increase this number for more rounds of dialogue\n",
        "            response = self.user.provide_evidence()\n",
        "            print(response)\n",
        "            print(self.agent.generate_response(response))\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()\n",
        "\n",
        "# Create a user and an agent\n",
        "user = User(\"User\")\n",
        "agent = Agent(\"Agent\")\n",
        "\n",
        "# Create a chat\n",
        "chat = Chat(user, agent)\n",
        "\n",
        "# Start a persuasion dialogue\n",
        "chat.start_dialogue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTEYG2703F4-",
        "outputId": "eb0438ad-bfbd-41de-acf4-b3b221302639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User, what is your claim? global warming is a hoax\n",
            "User: global warming is a hoax\n",
            "Agent: Assistant: I understand that there are different opinions on global warming, but the scientific consensus is that it is a real and pressing issue. The overwhelming majority of climate scientists agree that human activities, such as burning fossil fuels, deforestation, and industrial processes, are contributing to the warming of the Earth's climate. This consensus is based on extensive research, data analysis, and peer-reviewed studies. It is important to consider the evidence provided by experts in the field when discussing topics like global warming.\n",
            "User, what is your evidence? earth has seen periods of warming and cooling\n",
            "User: earth has seen periods of warming and cooling\n",
            "Agent: Assistant: That's correct! Earth's climate has gone through periods of warming and cooling throughout its history. These natural fluctuations occur over long periods of time and are influenced by a variety of factors, including changes in solar activity, volcanic activity, and fluctuations in greenhouse gas concentrations.\n",
            "\n",
            "These periods of warming and cooling are often referred to as climate cycles or climate oscillations. One well-known example is the ice age cycles, which involve alternating periods of glacial advances and retreats over tens of thousands of years.\n",
            "\n",
            "It's important to note that while natural climate cycles have occurred in the past, the current global warming trend is primarily attributed to human activities, particularly the burning of fossil fuels and deforestation. This has led to an increase in greenhouse gas concentrations, trapping more heat in the atmosphere and contributing to the ongoing global warming phenomenon.\n",
            "User, what is your evidence? ice ages occured\n",
            "User: ice ages occured\n",
            "Agent: Assistant: Ice ages occurred at various times throughout Earth's history. The most recent ice age, known as the Pleistocene Ice Age, began about 2.6 million years ago and ended roughly 11,700 years ago. During this period, large portions of the Earth were covered in ice sheets and glaciers.\n",
            "\n",
            "There have been multiple ice ages throughout geological time, with the most significant ones occurring around 300 million, 450 million, and 650 million years ago. These ice ages were characterized by the formation of extensive ice sheets and periods of glaciation, followed by interglacial periods of warmer climate.\n",
            "\n",
            "Ice ages are believed to be caused by a combination of factors, including variations in Earth's orbit around the Sun, changes in atmospheric composition, and feedback mechanisms involving ice and snow cover. These changes can result in a decrease in global temperatures, leading to the expansion of polar ice and the formation of ice sheets.\n",
            "\n",
            "Ice ages have had a significant impact on the Earth's landscape, shaping mountains, valleys, and lakes through the movement of glaciers. They have also influenced the distribution of plants and animals, with species adapting to survive in colder climates or migrating to regions more suitable to their needs.\n",
            "\n",
            "It's important to note that we are currently in an interglacial period, meaning we are in a relatively warm phase between ice ages. However, climate change is impacting the Earth's climate system, and there is ongoing scientific research on how this might influence future ice ages.\n"
          ]
        }
      ]
    }
  ]
}